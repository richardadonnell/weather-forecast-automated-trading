{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#!/usr/bin/env python\n", "# coding: utf-8"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import logging"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import matplotlib.pyplot as plt\n", "import numpy as np\n", "import pandas as pd"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.style.use(\"fivethirtyeight\")\n", "import datetime\n", "# Add these imports at the top\n", "import time\n", "import uuid\n", "from functools import wraps\n", "from typing import Any, Dict"]}, {"cell_type": "markdown", "metadata": {}, "source": ["API"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import requests\n", "import tensorflow as tf\n", "from bs4 import BeautifulSoup\n", "from sklearn.linear_model import Ridge\n", "from sklearn.metrics import mean_squared_error, r2_score\n", "# ML\n", "from sklearn.model_selection import GridSearchCV, train_test_split\n", "from sklearn.preprocessing import MinMaxScaler\n", "from tensorflow.keras.callbacks import EarlyStopping\n", "from tensorflow.keras.layers import LSTM, Dense\n", "from tensorflow.keras.models import Sequential"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Configure logging"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["logging.basicConfig(filename='output.log', level=logging.DEBUG, \n", "                    format='%(asctime)s - %(levelname)s - %(message)s')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Suppress TensorFlow warnings"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import os"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Suppress TensorFlow logging\n", "import tensorflow as tf"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["tf.get_logger().setLevel('ERROR')  # Suppress TensorFlow warnings"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Optional: Suppress CUDA warnings"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import warnings"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["warnings.filterwarnings('ignore', category=RuntimeWarning)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Configure GPU memory growth"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["gpus = tf.config.list_physical_devices('GPU')\n", "if gpus:\n", "    try:\n", "        for gpu in gpus:\n", "            tf.config.experimental.set_memory_growth(gpu, True)\n", "    except RuntimeError as e:\n", "        print(f\"GPU configuration error: {e}\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Data Gathering and Evaluation<br>\n", "I found 5 sources for daily weather data in New York City as follows:<br>\n", "1. National Centers for Environmental Information (NCEI)<br>\n", "2. National Weather Service<br>\n", "3. Visual Crossing<br>\n", "4. Meteomatics<br>\n", "5. Yahoo Weather"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### National Weather Service"]}, {"cell_type": "markdown", "metadata": {}, "source": ["I was initially thrilled about the this data source, as it closely aligns with what's behind Kalshi's weather event trading. However, upon delving into their API documentation, my excitement was met with a degree of disappointment. It became evident that while they only offer forecast weather data through their API, and the data available for direct download is confined to a-month record. This limitation prompted me to explore alternative data sources that better suit the needs."]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Define the URL for NYC data"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["url = \"https://forecast.weather.gov/MapClick.php?lat=40.714530000000025&lon=-74.00711999999999\""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["try:\n", "    # Send a GET request to the URL\n", "    response = requests.get(url)\n\n", "    # Check if the request was successful (status code 200)\n", "    if response.status_code == 200:\n", "        # Parse the HTML content\n", "        soup = BeautifulSoup(response.text, \"html.parser\")\n", "        observed_data_divs = soup.find_all(\"div\", class_=\"tombstone-container\")\n", "        timestamps = []\n", "        temperatures = []\n\n", "        # Extract data from API\n", "        for observed_div in observed_data_divs:\n", "            timestamp = observed_div.find(\"p\", class_=\"period-name\").get_text(strip=True)\n", "            temperature_element = observed_div.find(\"p\", class_=\"temp\")\n", "            if temperature_element:\n", "                temperature = temperature_element.get_text(strip=True)\n", "                timestamps.append(timestamp)\n", "                temperatures.append(temperature)\n", "            else:\n", "                logging.warning(f\"Temperature element not found for timestamp: {timestamp}\")\n\n", "        # Store data\n", "        nws_data = pd.DataFrame({\n", "            \"Timestamp\": timestamps,\n", "            \"Temperature (Fahrenheit)\": temperatures\n", "        })\n", "        logging.info(\"API request successful\")\n", "    else:\n", "        logging.error(f\"API request failed with status code {response.status_code}\")\n", "except requests.exceptions.RequestException as e:\n", "    logging.error(f\"Error occurred during API request: {str(e)}\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### National Centers for Environmental Information (NCEI)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[405]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Add these imports at the top of the file"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import urllib.parse\n", "from datetime import date, datetime, timedelta"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Update the NCEI API section"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def get_ncei_data(start_date: date, end_date: date) -> pd.DataFrame:\n", "    \"\"\"\n", "    Fetch weather data from NCEI API for the given date range with improved error handling.\n", "    \"\"\"\n", "    NCEI_API_KEY = \"hQjOAltlsPnryPJlIEkjkzQqJFPtGOpe\"\n", "    NCEI_BASE_URL = \"https://www.ncdc.noaa.gov/cdo-web/api/v2/data\"\n", "    NCEI_STATION_ID = \"GHCND:USW00094728\"  # Central Park Station\n", "    \n", "    api_data = []\n", "    current_date = start_date\n", "    \n", "    while current_date <= end_date:\n", "        # Request smaller chunks (180 days) to reduce load and probability of failure\n", "        period_end = min(current_date + timedelta(days=180), end_date)\n", "        offset = 0\n", "        max_retries = 3\n", "        \n", "        while True:  # Loop for pagination\n", "            params = {\n", "                \"datasetid\": \"GHCND\",\n", "                \"stationid\": NCEI_STATION_ID,\n", "                \"startdate\": current_date.strftime(\"%Y-%m-%d\"),\n", "                \"enddate\": period_end.strftime(\"%Y-%m-%d\"),\n", "                \"units\": \"standard\",\n", "                \"limit\": 1000,\n", "                \"offset\": offset\n", "            }\n", "            \n", "            headers = {\n", "                \"token\": NCEI_API_KEY\n", "            }\n", "            \n", "            retry_count = 0\n", "            success = False\n", "            \n", "            while retry_count < max_retries and not success:\n", "                try:\n", "                    logging.info(f\"Requesting data for period {current_date} to {period_end}, offset {offset}\")\n", "                    response = requests.get(NCEI_BASE_URL, params=params, headers=headers)\n", "                    \n", "                    if response.status_code == 200:\n", "                        data = response.json()\n", "                        if \"results\" in data and data[\"results\"]:\n", "                            # Remove duplicates before extending\n", "                            new_results = data[\"results\"]\n", "                            existing_dates = {(d['date'], d['datatype']) for d in api_data}\n", "                            unique_results = [\n", "                                r for r in new_results \n", "                                if (r['date'], r['datatype']) not in existing_dates\n", "                            ]\n", "                            api_data.extend(unique_results)\n", "                            logging.info(f\"Retrieved {len(unique_results)} new records\")\n", "                            \n", "                            # Check if we need to get more records\n", "                            if len(data[\"results\"]) < 1000:\n", "                                success = True\n", "                                break  # Exit retry loop and pagination loop\n", "                            offset += 1000  # Move to next page\n", "                            success = True  # Success for this page\n", "                        else:\n", "                            logging.info(f\"No results found for period {current_date} to {period_end}\")\n", "                            success = True\n", "                            break  # Exit retry loop and pagination loop\n", "                            \n", "                    elif response.status_code == 429:  # Too many requests\n", "                        logging.warning(\"Rate limit exceeded, waiting 60 seconds...\")\n", "                        time.sleep(60)\n", "                        retry_count += 1\n", "                    elif response.status_code == 503:  # Service unavailable\n", "                        logging.warning(f\"Service unavailable, attempt {retry_count + 1}/{max_retries}\")\n", "                        time.sleep(10 * (retry_count + 1))  # Exponential backoff\n", "                        retry_count += 1\n", "                    else:\n", "                        logging.error(f\"API request failed with status code {response.status_code}\")\n", "                        retry_count += 1\n", "                    \n", "                except requests.exceptions.RequestException as e:\n", "                    logging.error(f\"API request failed: {str(e)}\")\n", "                    retry_count += 1\n", "                    if retry_count < max_retries:\n", "                        time.sleep(5 * (retry_count + 1))\n", "                \n", "            # If all retries failed for this offset, move to next date range\n", "            if not success:\n", "                logging.error(f\"Failed to retrieve data after {max_retries} attempts\")\n", "                break\n", "                \n", "            # If we didn't get a full page of results, move to next date range\n", "            if success and len(data.get(\"results\", [])) < 1000:\n", "                break\n", "                \n", "            # Rate limiting\n", "            time.sleep(0.2)\n", "            \n", "        # Move to next date range\n", "        current_date = period_end + timedelta(days=1)\n", "    \n", "    if not api_data:\n", "        logging.error(\"No data retrieved from NCEI API\")\n", "        return pd.DataFrame()\n", "    \n", "    # Create DataFrame and remove duplicates\n", "    df = pd.DataFrame(api_data)\n", "    df = df.drop_duplicates(subset=['date', 'datatype', 'value'])\n", "    \n", "    logging.info(f\"Retrieved total of {len(df)} records\")\n", "    return df"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Define ncei_processing function before using it"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def ncei_processing(df: pd.DataFrame) -> pd.DataFrame:\n", "    \"\"\"Process NCEI weather data into a clean DataFrame with improved duplicate handling\"\"\"\n", "    try:\n", "        if df.empty:\n", "            logging.error(\"Empty dataframe provided to ncei_processing\")\n", "            return pd.DataFrame()\n", "        \n", "        # Debug print to see column names\n", "        logging.debug(f\"Columns in input DataFrame: {df.columns.tolist()}\")\n", "        \n", "        # Remove duplicates before pivoting\n", "        df = df.drop_duplicates(subset=['date', 'datatype', 'value'])\n", "        \n", "        # Pivot with aggfunc to handle any remaining duplicates\n", "        processed_df = df.pivot_table(\n", "            index='date',\n", "            columns='datatype',\n", "            values='value',\n", "            aggfunc='first'  # Take first value if duplicates exist\n", "        ).reset_index()\n", "        \n", "        # Debug print after pivot\n", "        logging.debug(f\"Columns after pivot: {processed_df.columns.tolist()}\")\n", "        \n", "        # Rename columns to be descriptive\n", "        processed_df = processed_df.rename(columns={\n", "            'AWND': 'avg wind speed',\n", "            'PRCP': 'precipitation',\n", "            'SNOW': 'snowfall',\n", "            'SNWD': 'snow depth',\n", "            'TMAX': 'max temp',\n", "            'TMIN': 'min temp',\n", "            'WT01': 'fog',\n", "            'WT03': 'thunder',\n", "            'WT08': 'smoke/haze'\n", "        })\n\n", "        # Process NaN values\n", "        processed_df = processed_df.dropna(subset=['max temp'])\n", "        processed_df = processed_df.fillna(0)\n\n", "        # Extract date elements\n", "        processed_df['date'] = pd.to_datetime(processed_df['date'])\n", "        processed_df['year'] = processed_df['date'].dt.year\n", "        processed_df['month'] = processed_df['date'].dt.month\n", "        processed_df['day'] = processed_df['date'].dt.day\n", "        processed_df['quarter'] = processed_df['date'].dt.quarter\n", "        \n", "        logging.debug(f\"Processed NCEI DataFrame: {processed_df.head()}\")\n", "        return processed_df\n", "        \n", "    except Exception as e:\n", "        logging.error(f\"Error in ncei_processing: {str(e)}\")\n", "        return pd.DataFrame()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Main execution block"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["try:\n", "    # Define date range for data collection\n", "    start_date = datetime.strptime(\"2019-01-01\", \"%Y-%m-%d\").date()\n", "    end_date = datetime.now().date()\n", "    \n", "    # Get data from NCEI API\n", "    api_data = get_ncei_data(start_date, end_date)\n", "    \n", "    if not api_data.empty:\n", "        # Process the data using ncei_processing function\n", "        ncei_df = ncei_processing(api_data)\n", "        \n", "        if not ncei_df.empty:\n", "            logging.info(\"Successfully processed NCEI data\")\n", "            \n", "            # Check if 'date' column exists before setting index\n", "            if 'date' in ncei_df.columns:\n", "                ncei_df.set_index('date', inplace=True)\n", "                \n", "                # Visualize the temperature data\n", "                plt.figure(figsize=(14, 5))\n", "                plt.plot(ncei_df.index, ncei_df['max temp'], marker='o', linestyle='-', color='b')\n", "                plt.title('New York Max Temperature (2019 - 2024)')\n", "                plt.xlabel('Date')\n", "                plt.ylabel('Temperature (\u00b0F)')\n", "                plt.grid(True)\n", "                plt.show()\n", "            else:\n", "                logging.error(f\"'date' column not found. Available columns: {ncei_df.columns.tolist()}\")\n", "        else:\n", "            logging.error(\"Failed to process NCEI data\")\n", "    else:\n", "        logging.error(\"No data retrieved from NCEI API\")\n", "        \n", "except Exception as e:\n", "    logging.error(f\"Error in main execution: {str(e)}\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Visual Crossing"]}, {"cell_type": "markdown", "metadata": {}, "source": ["I attempted to pull the data from Visual Crossing's API, but unfortunately there're limits on the amount of records per request and on the number of calls every day. I ended up having to download csv data from their website."]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[406]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Define Visual Crossing API constants"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["VC_API_KEY = \"WS8SPUNTV45687SM8PE4SP8EC\"  # Your API key\n", "VC_BASE_URL = \"https://weather.visualcrossing.com/VisualCrossingWebServices/rest/services/timeline\""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def get_visual_crossing_data(start_date: str, end_date: str) -> pd.DataFrame:\n", "    \"\"\"\n", "    Get weather data from Visual Crossing API with proper rate limiting and chunking.\n", "    Uses Timeline API endpoint with optimized query parameters.\n", "    \"\"\"\n", "    VC_API_KEY = \"WS8SPUNTV45687SM8PE4SP8EC\"\n", "    VC_BASE_URL = \"https://weather.visualcrossing.com/VisualCrossingWebServices/rest/services/timeline\"\n", "    \n", "    all_data = []\n", "    current_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n", "    end_datetime = datetime.strptime(end_date, \"%Y-%m-%d\")\n", "    \n", "    # Reduce data points by only requesting daily data\n", "    params = {\n", "        \"unitGroup\": \"us\",\n", "        \"include\": \"days\",  # Only include daily data, not hourly\n", "        \"key\": VC_API_KEY,\n", "        \"contentType\": \"json\",\n", "        \"elements\": \"datetime,tempmax,tempmin,temp,humidity,precip,windspeed\"  # Only request needed elements\n", "    }\n", "    \n", "    # Use smaller chunks (90 days) to stay well under the 1000 record limit\n", "    chunk_size = 90\n", "    retry_count = 0\n", "    max_retries = 3\n", "    \n", "    while current_date <= end_datetime:\n", "        chunk_end = min(current_date + timedelta(days=chunk_size), end_datetime)\n", "        \n", "        location = \"New York City,USA\"\n", "        url = f\"{VC_BASE_URL}/{urllib.parse.quote(location)}/{current_date.strftime('%Y-%m-%d')}/{chunk_end.strftime('%Y-%m-%d')}\"\n", "        \n", "        success = False\n", "        while not success and retry_count < max_retries:\n", "            try:\n", "                logging.info(f\"Requesting data for period {current_date.strftime('%Y-%m-%d')} to {chunk_end.strftime('%Y-%m-%d')}\")\n", "                response = requests.get(url, params=params)\n", "                \n", "                if response.status_code == 200:\n", "                    data = response.json()\n", "                    if 'days' in data:\n", "                        chunk_df = pd.DataFrame(data['days'])\n", "                        all_data.append(chunk_df)\n", "                        logging.info(f\"Successfully retrieved {len(chunk_df)} days of data\")\n", "                        success = True\n", "                    else:\n", "                        logging.warning(f\"No daily data found in response for period {current_date} to {chunk_end}\")\n", "                        success = True  # Consider it a success to move to next chunk\n", "                        \n", "                elif response.status_code == 429:  # Rate limit exceeded\n", "                    retry_count += 1\n", "                    wait_time = min(60 * retry_count, 300)  # Exponential backoff up to 5 minutes\n", "                    logging.warning(f\"Rate limit exceeded, waiting {wait_time} seconds... (Attempt {retry_count}/{max_retries})\")\n", "                    time.sleep(wait_time)\n", "                    \n", "                else:\n", "                    logging.error(f\"API request failed with status code {response.status_code}: {response.text}\")\n", "                    retry_count += 1\n", "                    time.sleep(10)  # Wait before retry\n", "                    \n", "            except requests.exceptions.RequestException as e:\n", "                logging.error(f\"API request failed: {str(e)}\")\n", "                retry_count += 1\n", "                time.sleep(10)\n", "                \n", "        if not success:\n", "            logging.error(f\"Failed to retrieve data after {max_retries} attempts\")\n", "            break\n", "            \n", "        # Reset retry count for next chunk\n", "        retry_count = 0\n", "        \n", "        # Move to next chunk\n", "        current_date = chunk_end + timedelta(days=1)\n", "        \n", "        # Rate limiting - minimum 1 second between requests\n", "        time.sleep(1)\n", "    \n", "    if not all_data:\n", "        logging.error(\"No data retrieved from Visual Crossing API\")\n", "        return pd.DataFrame()\n", "        \n", "    # Combine all chunks\n", "    df = pd.concat(all_data, ignore_index=True)\n", "    \n", "    # Try to read from CSV if API fails\n", "    if df.empty:\n", "        logging.info(\"Falling back to CSV file\")\n", "        try:\n", "            df = pd.read_csv(\"visual_crossing.csv\")\n", "            if df.empty:\n", "                logging.error(\"Visual Crossing CSV file is empty\")\n", "            else:\n", "                df['datetime'] = pd.to_datetime(df['datetime'])\n", "        except FileNotFoundError:\n", "            logging.error(\"Visual Crossing CSV file not found\")\n", "        except Exception as e:\n", "            logging.error(f\"Error reading Visual Crossing CSV: {str(e)}\")\n", "    \n", "    return df"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Usage"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["try:\n", "    # Request smaller date range to stay within limits\n", "    start_date = \"2023-01-01\"  # More recent date range\n", "    end_date = \"2023-12-31\"    # One year of data\n", "    \n", "    vs_df = get_visual_crossing_data(start_date, end_date)\n", "    \n", "    if not vs_df.empty:\n", "        # Process the DataFrame\n", "        vs_df['datetime'] = pd.to_datetime(vs_df['datetime'])\n", "        vs_df = vs_df.set_index('datetime')\n", "        \n", "        # Create visualization\n", "        plt.figure(figsize=(14, 6))\n", "        plt.plot(vs_df.index, vs_df['tempmax'], marker='o', linestyle='-', color='b')\n", "        plt.title('New York Max Temperature (2023)')\n", "        plt.xlabel('Date')\n", "        plt.ylabel('Temperature (\u00b0F)')\n", "        plt.grid(True)\n", "        plt.show()\n", "    else:\n", "        logging.error(\"Failed to obtain weather data from both API and CSV\")\n", "        \n", "except Exception as e:\n", "    logging.error(f\"Error processing weather data: {str(e)}\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Yahoo Weather"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Yahoo Weather does not allow me to scrape historical data. They mostly have just forecast data."]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[153]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["try:\n", "    yahoo = pd.read_html(\"https://www.yahoo.com/news/weather/united-states/new-york/new-york-2459115\")\n", "    if yahoo:  # Check if any tables were found\n", "        print(\"Yahoo Weather Data:\")\n", "        print(yahoo)\n", "    else:\n", "        logging.warning(\"No weather data tables found on Yahoo Weather page\")\n", "except Exception as e:\n", "    logging.error(f\"Error fetching Yahoo Weather data: {str(e)}\")\n", "    yahoo = None"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Gathering relevant weather data to predict the daily maximum temperature in New York was a challenging endeavor. Identifying credible and dependable data sources was not easy. Once I had pinpointed these sources, I encountered further obstacles in scraping or downloading the data. The insufficiency of some API documentation added a layer of complexity, as understanding how to extract the right data often demanded a significant investment of time and effort.<br>\n", "<br>\n", "I faced numerous hurdles, such as locating station ID numbers, circumventing the limitations on API calls, and, in some cases, having to consider the costs associated with certain APIs. While it may seem that there is a plethora of weather data available, it became clear that only a handful of sources provided data that was truly usable for my prediction task. Some sources offered solely monthly or historical data with a limited time frame, while others provided only forecast data. <br>\n", "<br>\n", "After a thorough assessment of these challenges and limitations, I decided to rely on data from source NCEI as my foundation for processing and predicting daily maximum temperatures because it is the most consistent dataset. The only downside I have noticed is that the data does not update daily as expected, especially when we approach weekends or holidays."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Data Processing"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### NCEI Data"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[408]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Set 'Date' as the index"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["ncei_df.set_index('date', inplace=True)\n", "logging.debug(f\"NCEI DataFrame with 'date' as index: {ncei_df.head()}\")\n", "# # Visualize the data\n", "plt.figure(figsize=(14, 5))\n", "plt.plot(ncei_df.index, ncei_df['max temp'], marker='o', linestyle='-', color='b')\n", "plt.title('New York Max Temperature (2019 - 2023)')\n", "plt.xlabel('Date')\n", "plt.ylabel('Temperature (\u00b0F)')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(14, 5))\n", "plt.plot(ncei_df.index, ncei_df['precipitation'], marker='o', linestyle='-', color='b')\n", "plt.title('New York Daily Precipitation (2019 - 2023)')\n", "plt.xlabel('Date')\n", "plt.ylabel('Temperature (\u00b0F)')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Visual Crossing Data"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[410]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Drop NaN values"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["vs_df = vs_df.dropna()\n", "logging.debug(f\"Visual Crossing DataFrame after dropping NaNs: {vs_df.head()}\")\n", "# Set 'Date' as the index (required for time series plotting)\n", "vs_df.set_index('datetime', inplace=True)\n", "# Create the time series plot\n", "plt.figure(figsize=(14, 6))\n", "plt.plot(vs_df.index, vs_df['tempmax'], marker='o', linestyle='-', color='b')\n", "plt.title('New York Max Temperature (2022 - 2023)')\n", "plt.xlabel('Date')\n", "plt.ylabel('Temperature (\u00b0F)')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Model Training and Selection<br>\n", "### Linear Regression"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Visual Crossing Data"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[411]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Shift the max temp by one row to predict next day's max temperature"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["vs_df_shift = vs_df.copy()\n", "vs_df_shift['tempmax_next'] = vs_df_shift['tempmax'].shift(-1)\n", "logging.debug(f\"Visual Crossing DataFrame after shifting 'tempmax': {vs_df_shift.head()}\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Remove the first and last row because of the shift"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["vs_df_shift = vs_df_shift.iloc[1:-1]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Define features (x) and the target variable (y)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["x = vs_df_shift[['tempmax', 'dew', 'humidity', 'precip', 'windgust', 'windspeed', 'sealevelpressure', 'solarradiation', 'solarenergy']]\n", "y = vs_df_shift['tempmax_next']"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Split the data into training and testing sets"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Grid search for optimal parameters for a ridge linear regression model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["param_grid = {'alpha': [0.1, 1, 10]}\n", "grid_search = GridSearchCV(Ridge(), param_grid, cv=5)\n", "grid_search.fit(x_train, y_train)\n", "best_alpha = grid_search.best_params_['alpha']"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Fit the model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["best_model = Ridge(alpha=best_alpha)\n", "best_model.fit(x_train, y_train)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Make predictions on the test data"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["y_pred = best_model.predict(x_test)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Evaluate the model's performance"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["mse = mean_squared_error(y_test, y_pred)\n", "rmse = np.sqrt(mse)\n", "r2 = r2_score(y_test, y_pred)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(f'Mean Squared Error: {mse}')\n", "print(f'Root Mean Squared Error: {rmse}')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Visualize the actual vs. predicted values"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.scatter(y_test, y_pred)\n", "plt.xlabel('Actual Max Temperature (\u00b0F)')\n", "plt.ylabel('Predicted Max Temperature (\u00b0F)')\n", "plt.title('Actual vs. Predicted Max Temperature')\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### NCEI Data"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[412]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Shift the max temp by one row to predict next day's max temperature"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["ncei_df_shift = ncei_df.copy()\n", "ncei_df_shift['max temp next'] = ncei_df_shift['max temp'].shift(-1)\n", "logging.debug(f\"NCEI DataFrame after shifting 'max temp': {ncei_df_shift.head()}\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Remove the first and last row because of the shift"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["ncei_df_shift = ncei_df_shift.iloc[1:-1]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Define features (x) and the target variable (y)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["x = ncei_df_shift[['avg wind speed','precipitation','snowfall','snow depth','min temp','max temp','fog','thunder','smoke/haze']]\n", "y = ncei_df_shift['max temp next']"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Split the data into training and testing sets"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Grid search for optimal parameters for a ridge linear regression model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["param_grid = {'alpha': [0.1, 1, 10]}\n", "grid_search = GridSearchCV(Ridge(), param_grid, cv=5)\n", "grid_search.fit(x_train, y_train)\n", "best_alpha = grid_search.best_params_['alpha']\n", "print(f'The best alpha is {best_alpha}')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Fit the model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["best_model = Ridge(alpha=best_alpha)\n", "best_model.fit(x_train, y_train)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Make predictions on the test data"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["y_pred = best_model.predict(x_test)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Evaluate the model's performance"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["mse = mean_squared_error(y_test, y_pred)\n", "rmse = np.sqrt(mse)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(f'Mean Squared Error: {mse}')\n", "print(f'Root Mean Squared Error: {rmse}')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Visualize the actual vs. predicted values"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.scatter(y_test, y_pred)\n", "plt.xlabel('Actual Max Temperature (\u00b0F)')\n", "plt.ylabel('Predicted Max Temperature (\u00b0F)')\n", "plt.title('Actual vs. Predicted Max Temperature')\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Building a robust linear regression model was a meticulous process that involved several critical steps. To ensure the model's optimal performance, I employed a grid search to fine-tune its parameters. This systematic search exhaustively explored all possible parameter combinations, allowing me to pinpoint the regularization term that delivered the most accurate results. Through the grid search, I found out that the optimal alpha was 10. "]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Long Short Term Memory (LSTM)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["From the above visualizations of the daily temperature, I realized that there is seasonality factor, and thought it would be appropriate to try implementing a Long Short Term Memory (LSTM) modelfor the following reasons:<br>\n", "1. Weather data is inherently sequential, where past conditions can significantly impact future conditions. LSTM can handle sequential data and capture long-range dependencies. They can effectively model the temporal relationships in weather data..<br>\n", "<br>\n", "2. LSTM can automatically learn relevant features from the data, reducing the need for manual feature engineering. They can extract complex patterns and relationships within the data, such as the impact of multiple weather variables on future conditions.<br>\n", "<br>\n", "3. Weather forecasting often involves multiple variables (e.g., temperature, precipitation, wind speed) that interact with each other. LSTM can handle multivariate time series data and model complex, nonlinear relationships in weather data."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Multivariate LSTM"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[418]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Extract the feature and target data"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["features = ncei_df[['avg wind speed','precipitation','snowfall','snow depth','min temp','max temp']].values.astype(float)\n", "target = ncei_df['max temp'].values.astype(float)\n", "logging.debug(f\"Features shape: {features.shape}, Target shape: {target.shape}\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Normalize the data"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["scaler = MinMaxScaler(feature_range=(0, 1))\n", "scaled_features = scaler.fit_transform(features)\n", "scaled_target = scaler.fit_transform(target.reshape(-1, 1))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Define the number of time steps and features"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["n_steps = 30\n", "n_features = features.shape[1]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Create sequences for training"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["x, y = [], []\n", "for i in range(len(ncei_df) - n_steps):\n", "    x.append(scaled_features[i:i + n_steps, :])\n", "    y.append(scaled_target[i + n_steps])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["x, y = np.array(x), np.array(y)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Split the data into training, validation and testing sets"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["train_size = int(0.8 * len(ncei_df))\n", "val_size = int(0.9 * len(ncei_df))\n", "x_train, x_val, x_test = x[:train_size], x[train_size:val_size], x[val_size:]\n", "y_train, y_val, y_test =  y[:train_size], y[train_size:val_size], y[val_size:]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Build the LSTM model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model = Sequential()\n", "model.add(LSTM(64, input_shape=(n_steps, n_features)))\n", "model.add(Dense(16, activation='relu'))\n", "model.add(Dense(1))\n", "model.compile(optimizer='adam', loss='mse', metrics='mean_squared_error')\n", "model.summary()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In training these LSTM models, I monitored validation loss and stopped training when the metric starts to degrade or level off. Here are the reasons why I implemented early stopping:<br>\n", "1. Preventing Overfitting: Early stopping helps by halting training when the model's performance on a validation dataset starts to degrade, indicating overfitting.<br>\n", "<br>\n", "2. Optimizing Training Time: Training LSTMs can be computationally expensive and time-consuming. Early stopping allows me to save time and resources by avoiding unnecessary training epochs. When the model reaches an optimal level of performance, the model will stop training early rather than running for a fixed number of epochs.<br>\n", "<br>\n", "3. Automating Model Selection: Early stopping automates the process of selecting the optimal number of training epochs. Instead of manually specifying the number of epochs, early stopping dynamically determines the stopping point based on the model's performance."]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[419]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Set up early stopping"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["early_stopping = EarlyStopping(monitor='val_loss',\n", "                               patience=5,         \n", "                               restore_best_weights=True) "]}, {"cell_type": "markdown", "metadata": {}, "source": ["Train the model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["history = model.fit(x_train, y_train, epochs=100, batch_size=32, validation_data=(x_val, y_val), callbacks=[early_stopping])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[420]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Create a range of epochs"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["best_epoch = 51\n", "epochs = range(1, best_epoch+1)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Plot training and validation loss"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["train_loss = history.history['loss']\n", "val_loss = history.history['val_loss']"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.plot(epochs, train_loss, label='Training Loss')\n", "plt.plot(epochs, val_loss, label='Validation Loss')\n", "plt.xlabel('Epochs')\n", "plt.ylabel('Loss')\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[421]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Make predictions"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["y_pred = model.predict(x_test)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Inverse transform the predictions to get real values"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["y_pred = scaler.inverse_transform(y_pred)\n", "y_test = scaler.inverse_transform(y_test.reshape(-1, 1))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Calculate RMSE"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n", "print(f\"Root Mean Squared Error (RMSE): {rmse}\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Plot actual vs. predicted values"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(12, 6))\n", "plt.plot(y_test, label='Actual')\n", "plt.plot(y_pred, label='Predicted')\n", "plt.legend()\n", "plt.xlabel('Time Step')\n", "plt.ylabel('Temperature (\u00b0F)')\n", "plt.title('Daily Max Temperature Forecasting with Multivariate LSTM')\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Univariate LSTM"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[422]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Extract the temperature values and convert them to an array"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["temperatures = ncei_df['max temp'].values.astype(float)\n", "logging.debug(f\"Temperatures shape: {temperatures.shape}\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Normalize the data to be in the range [0, 1]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["scaler = MinMaxScaler(feature_range=(0, 1))\n", "temperatures = scaler.fit_transform(temperatures.reshape(-1, 1))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Define a function to create sequences for training the LSTM model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def create_sequences(data, seq_length):\n", "    x, y = [], []\n", "    for i in range(len(data) - seq_length):\n", "        x.append(data[i:i + seq_length])\n", "        y.append(data[i + seq_length])\n", "    return np.array(x), np.array(y)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Set the sequence length (number of past days to consider for prediction)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["seq_length = 30"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Create sequences for training"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["x, y = create_sequences(temperatures, seq_length)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Split the data into training (80%), validation (10%) and testing sets (10%)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["train_size = int(0.8 * len(ncei_df))\n", "val_size = int(0.9 * len(ncei_df))\n", "x_train, x_val, x_test = x[:train_size], x[train_size:val_size], x[val_size:]\n", "y_train, y_val, y_test =  y[:train_size], y[train_size:val_size], y[val_size:]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Build the LSTM model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["lstm_model = Sequential()\n", "lstm_model.add(LSTM(64, input_shape=(seq_length, 1)))\n", "lstm_model.add(Dense(16, activation='relu'))\n", "lstm_model.add(Dense(1))\n", "lstm_model.compile(optimizer='adam', loss='mse', metrics='mean_squared_error')\n", "lstm_model.summary()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[423]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Set up early stopping"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["early_stopping = EarlyStopping(monitor='val_loss',\n", "                               patience=5,         \n", "                               restore_best_weights=True) \n", "# Train the model\n", "history = lstm_model.fit(x_train, y_train, epochs=100, batch_size=32, validation_data=(x_val, y_val), callbacks=[early_stopping])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["best_epoch = 40\n", "epochs = range(1, best_epoch+1)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Plot training and validation loss"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["train_loss = history.history['loss']\n", "val_loss = history.history['val_loss']"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.plot(epochs, train_loss, label='Training Loss')\n", "plt.plot(epochs, val_loss, label='Validation Loss')\n", "plt.xlabel('Epochs')\n", "plt.ylabel('Loss')\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Make predictions on the test set"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["y_pred = lstm_model.predict(x_test)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Inverse transform the scaled predictions to get actual temperature values"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["y_test = scaler.inverse_transform(y_test.reshape(-1, 1))\n", "y_pred = scaler.inverse_transform(y_pred)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Calculate the Root Mean Squared Error (RMSE) as a measure of prediction accuracy"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n", "print(f\"Root Mean Squared Error (RMSE): {rmse}\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Plot the actual vs. predicted temperatures"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.figure(figsize=(12, 6))\n", "plt.plot(y_test, label='Actual Temperature')\n", "plt.plot(y_pred, label='Predicted Temperature')\n", "plt.legend()\n", "plt.xlabel('Day')\n", "plt.ylabel('Temperature (\u00b0F)')\n", "plt.title('Daily Max Temperature Prediction with Univariate LSTM')\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["After rigorous experimentation and training linear regression, univariate LSTM, and multivariate LSTM, I decided to choose the univariate LSTM model that was trained on NCEI's data because it exhibited superior performance. Beyond the metrics, the decision was also informed by the qualities of the data itself. NCEI's data presented as the more dependable and consistent source of information, enabling the machine learning to make more accurate and reliable predictions. Furthermore, the accessibility of NCEI's data, available for extraction without additional cost makes it a better option than VS's data."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Kalshi's API Trading"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Predict today's maximum temperature"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Get recent NCEI's data to predict"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["start_date = datetime.datetime.strptime(\"2024-10-01\", \"%Y-%m-%d\").date()\n", "last_date = datetime.date.today() - datetime.timedelta(days=1)\n", "api_recent = pd.DataFrame()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["while start_date <= last_date:\n", "    end_date = min(start_date + datetime.timedelta(days=100), last_date)\n", "    params = {\n", "        \"datasetid\": \"GHCND\",\n", "        \"stationid\": \"GHCND:USW00094728\",\n", "        \"startdate\": start_date,\n", "        \"enddate\": end_date,\n", "        \"units\": \"standard\",\n", "        \"datatypeid\": \"AWND,PRCP,SNOW,SNWD,TMAX,TMIN,WT01,WT03,WT08\",\n", "        \"limit\": 1000\n", "    }\n", "    # API request\n", "    response = requests.get(url, params=params, headers={\"token\": \"hQjOAltlsPnryPJlIEkjkzQqJFPtGOpe\"})\n", "    start_date = end_date + datetime.timedelta(days=1)\n", "    if response.status_code == 200:\n", "        print(\"API request successful\")\n", "        # Convert JSON to DataFrame\n", "        json_data = pd.DataFrame(response.json()[\"results\"])\n", "        result = pd.DataFrame(json_data)\n", "        api_recent = api_recent.append(json_data)\n", "    else:\n", "        print(f\"API request failed with status code {response.status_code}\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["ncei_df_recent = ncei_processing(api_recent)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Extract the temperature values and convert them to an array"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["temperatures_recent = ncei_df_recent['max temp'].values.astype(float)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Normalize the data to be in the range [0, 1]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["scaler = MinMaxScaler(feature_range=(0, 1))\n", "temperatures_recent = scaler.fit_transform(temperatures_recent.reshape(-1, 1))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Create sequences for training"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["x, y = create_sequences(temperatures_recent, seq_length)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Predict tomorrow's temperature"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["y_pred_recent = lstm_model.predict(x)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Inverse transform the predicted value to get the actual temperature"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["y_pred_recent = scaler.inverse_transform(y_pred_recent)\n", "y_pred_today = y_pred_recent[-1]\n", "print(f\"The maximum temperature today is: {y_pred_today}\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Implement Kalshi's API to trade"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In[ ]:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Add these imports at the top"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import time\n", "import uuid\n", "from functools import wraps\n", "from typing import Any, Dict"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Custom exceptions"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class KalshiAPIError(Exception):\n", "    def __init__(self, status_code, message):\n", "        self.status_code = status_code\n", "        self.message = message\n", "        super().__init__(f\"Kalshi API Error {status_code}: {message}\")\n", "        logging.error(f\"Kalshi API Error {status_code}: {message}\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Rate limiter class"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class RateLimiter:\n", "    def __init__(self, requests_per_second=10):\n", "        self.requests_per_second = requests_per_second\n", "        self.last_request_time = 0\n", "        \n", "    def wait_if_needed(self):\n", "        current_time = time.time()\n", "        time_since_last = current_time - self.last_request_time\n", "        if time_since_last < (1.0 / self.requests_per_second):\n", "            time.sleep((1.0 / self.requests_per_second) - time_since_last)\n", "        self.last_request_time = time.time()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Helper functions"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def get_headers(token: str) -> Dict[str, str]:\n", "    \"\"\"Get standard headers required for authenticated requests\"\"\"\n", "    return {\n", "        \"Authorization\": f\"Bearer {token}\",\n", "        \"Content-Type\": \"application/json\",\n", "        \"Accept\": \"application/json\"\n", "    }"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def handle_response(response):\n", "    \"\"\"Handle API response and raise appropriate errors\"\"\"\n", "    if response.status_code == 200:\n", "        return response.json()\n", "    elif response.status_code == 401:\n", "        raise KalshiAPIError(401, \"Unauthorized - check credentials\")\n", "    elif response.status_code == 429:\n", "        raise KalshiAPIError(429, \"Rate limit exceeded\")\n", "    else:\n", "        raise KalshiAPIError(\n", "            response.status_code,\n", "            f\"API request failed: {response.text}\"\n", "        )"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Main Kalshi client class"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class KalshiClient:\n", "    def __init__(self, email: str, password: str, demo: bool = True):\n", "        self.email = email\n", "        self.password = password\n", "        self.demo = demo\n", "        self.base_url = \"https://demo-api.kalshi.co/trade-api/v2\" if demo else \"https://trading-api.kalshi.com/trade-api/v2\"\n", "        self.token = None\n", "        self.rate_limiter = RateLimiter()\n", "        self.login()\n", "    def login(self):\n", "        \"\"\"Login and get auth token\"\"\"\n", "        login_url = f\"{self.base_url}/login\"\n", "        login_data = {\n", "            \"email\": self.email,\n", "            \"password\": self.password\n", "        }\n", "        response = requests.post(login_url, json=login_data)\n", "        data = handle_response(response)\n", "        self.token = data[\"token\"]\n", "    def get_exchange_status(self):\n", "        \"\"\"Get exchange status\"\"\"\n", "        self.rate_limiter.wait_if_needed()\n", "        response = requests.get(\n", "            f\"{self.base_url}/exchange/status\",\n", "            headers=get_headers(self.token)\n", "        )\n", "        return handle_response(response)\n", "    def get_event(self, event_ticker: str):\n", "        \"\"\"Get event details\"\"\"\n", "        self.rate_limiter.wait_if_needed()\n", "        response = requests.get(\n", "            f\"{self.base_url}/events/{event_ticker}\",\n", "            headers=get_headers(self.token)\n", "        )\n", "        return handle_response(response)\n", "    def create_order(self, order_params: Dict[str, Any]):\n", "        \"\"\"Create a new order\"\"\"\n", "        # Validate required fields\n", "        required_fields = ['ticker', 'action', 'side', 'count', 'type']\n", "        for field in required_fields:\n", "            if field not in order_params:\n", "                raise ValueError(f\"Missing required field: {field}\")\n", "        \n", "        # Add client_order_id if not provided\n", "        if 'client_order_id' not in order_params:\n", "            order_params['client_order_id'] = str(uuid.uuid4())\n", "        self.rate_limiter.wait_if_needed()\n", "        response = requests.post(\n", "            f\"{self.base_url}/portfolio/orders\",\n", "            headers=get_headers(self.token),\n", "            json=order_params\n", "        )\n", "        return handle_response(response)\n", "    def get_market_prices(self, ticker: str):\n", "        \"\"\"Get current market prices for a ticker\"\"\"\n", "        self.rate_limiter.wait_if_needed()\n", "        response = requests.get(\n", "            f\"{self.base_url}/markets/{ticker}\",\n", "            headers=get_headers(self.token)\n", "        )\n", "        return handle_response(response)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Usage example"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def trade_temperature():\n", "    try:\n", "        # Initialize client\n", "        client = KalshiClient(\n", "            email=\"richardadonnell@gmail.com\",\n", "            password=\"akg_UYA-eqj4zqn7udv\",\n", "            demo=True\n", "        )\n\n", "        # Check exchange status\n", "        status = client.get_exchange_status()\n", "        logging.info(f\"Exchange status: {status}\")\n\n", "        # Get today's event ticker\n", "        today_date = datetime.date.today()\n", "        event_ticker = f'HIGHNY-{today_date.strftime(\"%y%b\").upper()}{today_date.strftime(\"%d\")}'\n\n", "        # Get event details\n", "        event = client.get_event(event_ticker)\n", "        markets = event.get('markets', [])\n", "        \n", "        if not markets:\n", "            raise ValueError(f\"No markets found for event {event_ticker}\")\n\n", "        # Process markets and create order\n", "        temp_list = []\n", "        market_tickers = []\n", "        for m in markets:\n", "            subtitle = m['subtitle'].split()\n", "            market_tickers.append(m['ticker'])\n", "            if \"or\" in subtitle:\n", "                temp_list.append(int(subtitle[0][:-1]))\n", "            else:\n", "                temp_list.append((int(subtitle[0][:-1]) + int(subtitle[-1][:-1])) / 2)\n\n", "        # Find market with furthest distance from prediction\n", "        i = np.argmax(abs(np.array(temp_list) - y_pred_today))\n", "        selected_ticker = market_tickers[i]\n\n", "        # Create order\n", "        order_params = {\n", "            'ticker': selected_ticker,\n", "            'type': 'market',\n", "            'action': 'buy',\n", "            'side': 'no',\n", "            'count': 10\n", "        }\n", "        order_response = client.create_order(order_params)\n", "        logging.info(f\"Order placed: {order_response}\")\n", "    except KalshiAPIError as e:\n", "        logging.error(f\"API Error: {e.message}\")\n", "    except Exception as e:\n", "        logging.error(f\"Error: {str(e)}\")"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}